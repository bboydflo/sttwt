{"version":3,"file":"index.mjs","sources":["../src/index.js"],"sourcesContent":["const WORD_BOUNDARY_CHARS = '\\t\\r\\n\\u00A0 â†µ!\\\"#$%&()*+,\\-.\\\\/:;<=>?@\\[\\\\\\]^_`{|}~'\nconst WORD_BOUNDARY_REGEX = new RegExp('[^' + WORD_BOUNDARY_CHARS + ']+', 'g')\n\n/**\n * simple tokenize a text into tokens.\n * @param {string} text to be tokenized. string must not be empty and has to contain non white space characters\n * @param {Regex} [customRegex] - custom regex to test for\n * @return {[{value: String, index: Number, offset: Number}]}\n */\nfunction tokenizeTextToWordTokens(text, customRegex) {\n  const tokens = []\n  // text does not contain non blank characters\n  if (typeof text !== 'string' || !text || !text.replace(/\\s*/, '')) {\n    return tokens\n  }\n  // customRegex is not a valid RegExp object\n  if (customRegex && Object.prototype.toString.call(customRegex) !== '[object RegExp]') {\n    return tokens\n  }\n  let myArray\n  while((myArray = (customRegex || WORD_BOUNDARY_REGEX).exec(text)) !== null) {\n    let value = myArray[0]\n    if (value && value.trim()) {\n      tokens.push({\n        value,\n        index: myArray.index,\n        offset: value.length\n      })\n    }\n  }\n  return tokens\n}\n\nexport default tokenizeTextToWordTokens\n"],"names":["const","WORD_BOUNDARY_REGEX","RegExp","text","customRegex","myArray","tokens","replace","Object","prototype","toString","call","exec","value","trim","push","index","offset","length"],"mappings":"AAAAA,IACMC,EAAsB,IAAIC,OAAO,kDAAmC,oBAQ1E,SAAkCC,EAAMC,OAUlCC,EATEC,EAAS,MAEK,iBAATH,IAAsBA,IAASA,EAAKI,QAAQ,MAAO,WACrDD,KAGLF,GAA+D,oBAAhDI,OAAOC,UAAUC,SAASC,KAAKP,UACzCE,OAG6D,QAA/DD,GAAWD,GAAeH,GAAqBW,KAAKT,KAAiB,KACtEU,EAAQR,EAAQ,GAChBQ,GAASA,EAAMC,QACjBR,EAAOS,KAAK,OACVF,EACAG,MAAOX,EAAQW,MACfC,OAAQJ,EAAMK,gBAIbZ"}